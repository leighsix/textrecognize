{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "import yaml\n",
    "import datetime\n",
    "import pytesseract as ocr\n",
    "from PIL import Image\n",
    "from src import pre_process as pp\n",
    "import os\n",
    "\n",
    "# configurations to read from YAML file\n",
    "configs = None\n",
    "def read_configs(config_file):\n",
    "    \"\"\" .yml file 을 읽어서 configuration 값의 객체를 갖습니다.\n",
    "    :param config_file:\n",
    "    :return: 읽은 configuration 을 담고있는 dictionary 형태로 반환\n",
    "    \"\"\"\n",
    "    # read contents from .yam config file\n",
    "    with open(config_file, 'r') as yml_file:\n",
    "        configurations = yaml.load(yml_file)  # use 'yaml' package to read .yml file\n",
    "\n",
    "    global configs  # global var : configs\n",
    "    configs = configurations  # set configs\n",
    "    return configurations  # return read configurations\n",
    "\n",
    "read_configs('C:/Users/Purple/textrecognition/config.yml')\n",
    "\n",
    "def recognize_text_from_file(image_path):\n",
    "    model_full_path = 'C:/Users/Purple/textrecognition/model/learning_graph.pb'\n",
    "    labels_full_path = 'C:/Users/Purple/textrecognition/model/learning_labels.txt'\n",
    "    # Read in the image_data\n",
    "    image_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
    "\n",
    "    # Loads label file, strips off carriage return\n",
    "    label_lines = [line.rstrip() for line\n",
    "                   in tf.gfile.GFile(labels_full_path)]\n",
    "\n",
    "    # Unpersists graph from file\n",
    "    with tf.gfile.FastGFile(model_full_path, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        _ = tf.import_graph_def(graph_def, name='')\n",
    "    \n",
    "    # Unpersists graph from file\n",
    "    with tf.Session() as sess:\n",
    "        # Feed the image_data as input to the graph and get first prediction\n",
    "        softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\n",
    "        predictions = sess.run(softmax_tensor, {'DecodeJpeg/contents:0': image_data})\n",
    "\n",
    "        # Sort to show labels of first prediction in order of confidence\n",
    "        top_k = predictions[0].argsort()[-len(predictions[0]):][::-1]\n",
    "\n",
    "        for node_id in top_k:\n",
    "            human_string = label_lines[node_id]\n",
    "            score = predictions[0][node_id]\n",
    "            print('%s (score = %.5f)' % (human_string, score))\n",
    "\n",
    "    # get most likely classification\n",
    "    answer = label_lines[top_k[0]]\n",
    "    return answer\n",
    "\n",
    "def get_language_from_file(image_path):\n",
    "    model_full_path = 'C:/Users/Purple/textrecognition/workspace/language_graph.pb'\n",
    "    labels_full_path = 'C:/Users/Purple/textrecognition/workspace/language_labels.txt'\n",
    "    # Read in the image_data\n",
    "    image_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
    "\n",
    "    # Loads label file, strips off carriage return\n",
    "    label_lines = [line.rstrip() for line\n",
    "                   in tf.gfile.GFile(labels_full_path)]\n",
    "\n",
    "    # Unpersists graph from file\n",
    "    with tf.gfile.FastGFile(model_full_path, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        _ = tf.import_graph_def(graph_def, name='')\n",
    "    \n",
    "    # Unpersists graph from file\n",
    "    with tf.Session() as sess:\n",
    "        # Feed the image_data as input to the graph and get first prediction\n",
    "        softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\n",
    "        predictions = sess.run(softmax_tensor, {'DecodeJpeg/contents:0': image_data})\n",
    "\n",
    "        # Sort to show labels of first prediction in order of confidence\n",
    "        top_k = predictions[0].argsort()[-len(predictions[0]):][::-1]\n",
    "\n",
    "        for node_id in top_k:\n",
    "            human_string = label_lines[node_id]\n",
    "            score = predictions[0][node_id]\n",
    "            print('%s (score = %.5f)' % (human_string, score))\n",
    "                # get most likely classification\n",
    "    answer = label_lines[top_k[0]]\n",
    "    return answer\n",
    "\n",
    "img = Image.open(open('C:/Users/Purple/textrecognition/chieng1.jpg', 'rb'))\n",
    "\n",
    "def get_text_from_file(img):\n",
    "    \"\"\" OCR 엔진(tesseract) 를 이용해 이미지에서 글자를 추출합니다.\n",
    "    :param image: 텍스트(Text)를 추출할 resource 이미지\n",
    "    :return: 추출한 텍스트(Text)를 String 형으로 반환\n",
    "    \"\"\"\n",
    "    # todo language 도 configs.yml file 에서 설정할 수 있도록 변경하기\n",
    "    if recognize_text_from_file(image) == 'text' :\n",
    "        if get_language_from_file(image) == 'eng' :\n",
    "            text = ocr.image_to_string(img, lang='eng')\n",
    "            print(text)\n",
    "        elif get_language_from_file(image) == 'chi' :\n",
    "            text = ocr.image_to_string(img, lang='chi')\n",
    "            print(text)\n",
    "    elif not(recognize_text_from_file(image) == 'text') :\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(open('C:/Users/Purple/textrecognition/chieng1.jpg', 'rb'))\n",
    "\n",
    "def get_text_from_file(img):\n",
    "    \"\"\" OCR 엔진(tesseract) 를 이용해 이미지에서 글자를 추출합니다.\n",
    "    :param image: 텍스트(Text)를 추출할 resource 이미지\n",
    "    :return: 추출한 텍스트(Text)를 String 형으로 반환\n",
    "    \"\"\"\n",
    "    # todo language 도 configs.yml file 에서 설정할 수 있도록 변경하기\n",
    "    if recognize_text_from_file(image) == 'text' :\n",
    "        if get_language_from_file(image) == 'eng' :\n",
    "            text = ocr.image_to_string(img, lang='eng')\n",
    "            print(text)\n",
    "        elif get_language_from_file(image) == 'chi' :\n",
    "            text = ocr.image_to_string(img, lang='chi')\n",
    "            print(text)\n",
    "    elif not(recognize_text_from_file(image) == 'text') :\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-206-bc8d2bc80ec0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_text_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-205-e5cd28c4f836>\u001b[0m in \u001b[0;36mget_text_from_file\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# todo language 도 configs.yml file 에서 설정할 수 있도록 변경하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mrecognize_text_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'text'\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mget_language_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'eng'\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mocr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'eng'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "get_text_from_file(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng (score = 0.77556)\n",
      "chi (score = 0.22444)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'eng'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_language_from_file('C:/Users/Purple/Desktop/english/DSC04255.JPGcrop_5_20180602030131.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]]], dtype=uint8)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_image('C:/Users/Purple/textrecognition/chieng1.jpg')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]]], dtype=uint8)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_from_image(image_path):\n",
    "    messages = []\n",
    "    cropped_images = process_image(image_path)\n",
    "    count = 1\n",
    "    for cropped in cropped_images:\n",
    "        count += 1\n",
    "        # gray_copy = pp.get_gray(cropped)\n",
    "        # gradient_copy = pp.get_gradient(gray_copy)\n",
    "        # gradient_copy = cv2.cvtColor(gradient_copy, cv2.COLOR_GRAY2BGR)\n",
    "        # answer = jt.get_answer_from_cv2_Image(gradient_copy)\n",
    "        # print(answer)\n",
    "        msg = get_text_from_file(cropped)\n",
    "        messages.append(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Purple\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:104: DeprecationWarning: path should be string, bytes, or os.PathLike, not numpy.ndarray\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-184-90dc78a63392>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mread_text_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Purple/textrecognition/chieng1.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-183-ab6f6d0b84fb>\u001b[0m in \u001b[0;36mread_text_from_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# answer = jt.get_answer_from_cv2_Image(gradient_copy)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# print(answer)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_text_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcropped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-139-51616a316cee>\u001b[0m in \u001b[0;36mget_text_from_file\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \"\"\"\n\u001b[0;32m    103\u001b[0m     \u001b[1;31m# todo language 도 configs.yml file 에서 설정할 수 있도록 변경하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrecognize_text_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'text'\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mget_language_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'eng'\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "read_text_from_image('C:/Users/Purple/textrecognition/chieng1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-90dc78a63392>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mread_text_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Purple/textrecognition/chieng1.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-148-9c0bdbc0c1e7>\u001b[0m in \u001b[0;36mread_text_from_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_text_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcropped_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcropped\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcropped_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\textrecognition\\src\\pre_process.py\u001b[0m in \u001b[0;36mprocess_image\u001b[1;34m(image_file)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[0mimage_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_gray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_origin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;31m# Morph Gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m     \u001b[0mimage_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_gray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m     \u001b[1;31m# Threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[0mimage_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_threshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_gradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\textrecognition\\src\\pre_process.py\u001b[0m in \u001b[0;36mget_gradient\u001b[1;34m(image_gray)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;31m# get configs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0mkernel_size_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gradient'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'kernel_size_row'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[0mkernel_size_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gradient'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'kernel_size_col'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;31m# make kernel matrix for dilation and erosion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Purple/textrecognition/config1.yml'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-94fdfabef2d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkernel_size_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"gradient\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"kernel_size_row\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "kernel_size_row = configs[\"gradient\"][\"kernel_size_row\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import pre_process as pp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def show_window(image, title='untitled', max_height=700):\n",
    "    \"\"\" 이미지 윈도우를 열어서 보여줍니다.\n",
    "\n",
    "    :param image: 보여줄 이미지 (OpenCV image 객체)\n",
    "    :param title: 윈도우 제목\n",
    "    :param max_height: 이미지 윈도우 사이즈의 최대 높이\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]  # get image size\n",
    "    if height > max_height:  # adjust window size if too large\n",
    "        rate = max_height / height\n",
    "        height = round(height * rate)\n",
    "        width = round(width * rate)  # apply the same rate to width\n",
    "\n",
    "    cv2.namedWindow(title, cv2.WINDOW_NORMAL)  # Create a window that the user can resize\n",
    "    cv2.resizeWindow(title, width, height)  # resize window according to the size of the image\n",
    "    cv2.imshow(title, image)  # open image window\n",
    "    key = cv2.waitKey(0)  # wait until keyboard input\n",
    "    cv2.destroyAllWindows()\n",
    "    return key\n",
    "\n",
    "\n",
    "def merge_horizontal(image_gray, image_bgr):\n",
    "    \"\"\" Height 사이즈가 같은 두 이미지를 옆으로(Horizontally) 병합 합니다.\n",
    "    이미지 처리(Image processing) 단계를 원본과 비교하기위한 목적으로,\n",
    "    2차원(2 dimension) 흑백 이미지와 3차원(3 dimension) BGR 컬리 이미지를 인자로 받아 병합합니다.\n",
    "\n",
    "    :param image_gray: 2차원(2 dimension) 흑백 이미지\n",
    "    :param image_bgr: 3차원(3 dimension) BGR 컬리 이미지\n",
    "    :return: 옆으로(Horizontally) 병합된 이미지\n",
    "    \"\"\"\n",
    "    # Make the grey scale image have 3 channels\n",
    "    image_cr = cv2.cvtColor(image_gray, cv2.COLOR_GRAY2BGR)\n",
    "    # Merge image horizontally\n",
    "    numpy_horizontal = np.hstack((image_cr, image_bgr))\n",
    "    # numpy_horizontal_concat = np.concatenate((image, image_contours), axis=1)\n",
    "    return numpy_horizontal\n",
    "\n",
    "\n",
    "def merge_vertical(image_gray, image_bgr):\n",
    "    \"\"\" Width 사이즈가 같은 두 이미지를 위아래로(Vertically) 병합 합니다.\n",
    "    이미지 처리(Image processing) 단계를 원본과 비교하기위한 목적으로,\n",
    "    2차원(2 dimension) 흑백 이미지와 3차원(3 dimension) BGR 컬리 이미지를 인자로 받아 병합합니다.\n",
    "\n",
    "    :param image_gray: 2차원(2 dimension) 흑백 이미지\n",
    "    :param image_bgr: 3차원(3 dimension) BGR 컬리 이미지\n",
    "    :return: 위아래로(Vertically) 병합된 이미지\n",
    "    \"\"\"\n",
    "    # Make the grey scale image have 3 channels\n",
    "    image_cr = cv2.cvtColor(image_gray, cv2.COLOR_GRAY2BGR)\n",
    "    # Merge image horizontally\n",
    "    numpy_vertical = np.vstack((image_cr, image_bgr))\n",
    "    return numpy_vertical\n",
    "\n",
    "\n",
    "def detect_line(image_binary):\n",
    "    \"\"\" 이미지에서 직선을 찾아서 초록색으로 표시한 결과를 반환합니다.\n",
    "\n",
    "    :param image_binary: 흑백(Binary) OpenCV image (2 dimension)\n",
    "    :return: 라인이 삭제된 이미지 (OpenCV image)\n",
    "    \"\"\"\n",
    "    copy = image_binary.copy()  # copy the image to be processed\n",
    "    copy_rbg = cv2.cvtColor(copy, cv2.COLOR_GRAY2RGB)\n",
    "    # get configs\n",
    "    threshold = pp.configs['remove_line']['threshold']\n",
    "    min_line_length = pp.configs['remove_line']['min_line_length']\n",
    "    max_line_gap = pp.configs['remove_line']['max_line_gap']\n",
    "\n",
    "    # fine and draw lines\n",
    "    lines = cv2.HoughLinesP(copy, 1, np.pi / 180, threshold, np.array([]), min_line_length, max_line_gap)\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]  # get end point of line : ( (x1, y1) , (x2, y2) )\n",
    "            # slop = 0\n",
    "            # if x2 != x1:\n",
    "            #     slop = abs((y2-y1) / (x2-x1))\n",
    "            # if slop < 0.5 or slop > 50 or x2 == x1:  # only vertical or parallel lines.\n",
    "            #  remove line drawing black line\n",
    "            cv2.line(copy_rbg, (x1, y1), (x2, y2), (0, 155, 0), 2)\n",
    "    return copy_rbg\n",
    "\n",
    "\n",
    "def get_step_compare_image(path_of_image):\n",
    "    \"\"\" 이미지 프로세싱 전 단계의 중간 결과물을 하나로 병합하여 반환합니다.\n",
    "\n",
    "    :param path_of_image:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # open original image\n",
    "    image_origin = pp.open_original(path_of_image)\n",
    "    # size up ( x4 )\n",
    "    image_origin = cv2.pyrUp(image_origin)\n",
    "    comparing_images = []\n",
    "\n",
    "    # Grey-Scale\n",
    "    image_gray = pp.get_gray(image_origin)\n",
    "    contours = pp.get_contours(image_gray)\n",
    "    image_with_contours = pp.draw_contour_rect(image_origin, contours)\n",
    "    # merge two image vertically\n",
    "    compare_set = merge_vertical(image_gray, image_with_contours)\n",
    "    comparing_images.append(compare_set)\n",
    "\n",
    "    # Morph Gradient\n",
    "    image_gradient = pp.get_gradient(image_gray)\n",
    "    # image_gradient = pp.get_canny(image_gray)\n",
    "    contours = pp.get_contours(image_gradient)\n",
    "    image_with_contours = pp.draw_contour_rect(image_origin, contours)\n",
    "    # merge two current step image vertically\n",
    "    compare_set = merge_vertical(image_gradient, image_with_contours)\n",
    "    comparing_images.append(compare_set)\n",
    "\n",
    "    # Threshold\n",
    "    image_threshold = pp.get_threshold(image_gradient)\n",
    "    contours = pp.get_contours(image_threshold)\n",
    "    image_with_contours = pp.draw_contour_rect(image_origin, contours)\n",
    "    # merge two image vertically\n",
    "    compare_set = merge_vertical(image_threshold, image_with_contours)\n",
    "    comparing_images.append(compare_set)\n",
    "\n",
    "    # Long line remove\n",
    "    image_line_removed = pp.remove_long_line(image_threshold)\n",
    "    contours = pp.get_contours(image_line_removed)\n",
    "    image_with_contours = pp.draw_contour_rect(image_origin, contours)\n",
    "    # merge two image vertically\n",
    "    compare_set = merge_vertical(image_line_removed, image_with_contours)\n",
    "    comparing_images.append(compare_set)\n",
    "\n",
    "    # Morph Close\n",
    "    image_close = pp.get_closing(image_line_removed)\n",
    "    contours = pp.get_contours(image_close)\n",
    "    image_with_contours = pp.draw_contour_rect(image_origin, contours)\n",
    "    # merge two image vertically\n",
    "    compare_set = merge_vertical(image_close, image_with_contours)\n",
    "    comparing_images.append(compare_set)\n",
    "\n",
    "    # Merge all step's images horizontally\n",
    "    image_merged_all = np.hstack(comparing_images)\n",
    "\n",
    "    return image_merged_all\n",
    "\n",
    "\n",
    "def get_image_with_contours(path_of_image):\n",
    "    \"\"\" 이미지 프로세싱을 거친 후,\n",
    "    최종적으로 얻은 Contours 를 원본 이미지 위에 그려서 반환합니다.\n",
    "\n",
    "    :param path_of_image:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # open original image\n",
    "    image_origin = pp.open_original(path_of_image)\n",
    "    # size up the resource ( x4 )\n",
    "    image_origin = cv2.pyrUp(image_origin)\n",
    "    # Grey-Scale\n",
    "    image_gray = pp.get_gray(image_origin)\n",
    "    # Morph Gradient\n",
    "    image_gradient = pp.get_gradient(image_gray)\n",
    "    # Threshold\n",
    "    image_threshold = pp.get_threshold(image_gradient)\n",
    "    # Long line remove\n",
    "    image_line_removed = pp.remove_long_line(image_threshold)\n",
    "    # Morph Close\n",
    "    image_close = pp.get_closing(image_line_removed)\n",
    "    # Get contours and Draw it on the original image\n",
    "    contours = pp.get_contours(image_close)\n",
    "    image_with_contours = pp.draw_contour_rect(image_origin, contours)\n",
    "    return image_with_contours\n",
    "\n",
    "\n",
    "def get_file_list(path):\n",
    "    \"\"\" path 가 가리키는 directory 의 모든 파일명을 읽어서 string 으로 반환합니다.\n",
    "    파일명은 Absolute path 가 포함된 이름입니다.\n",
    "\n",
    "    :param path: 읽어 들일 directory 의 절대경로\n",
    "    :return: directory 의 모든 file path 을 String 형으로 Array 에 담아 반환\n",
    "    \"\"\"\n",
    "    image_path_list = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        root_path = os.path.join(os.path.abspath(path), root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root_path, file)\n",
    "            image_path_list.append(file_path)\n",
    "\n",
    "    return image_path_list\n",
    "\n",
    "\n",
    "\n",
    "    return messages\n",
    "\n",
    "def get_image_with_lines(image_path):\n",
    "    image_origin = pp.open_original(image_path)\n",
    "    image_origin = cv2.pyrUp(image_origin)\n",
    "    # Grey-Scale\n",
    "    image_gray = pp.get_gray(image_origin)\n",
    "    # Morph Gradient\n",
    "    image_gradient = pp.get_gradient(image_gray)\n",
    "    # Threshold\n",
    "    image_threshold = pp.get_threshold(image_gradient)\n",
    "    # find and draw lines\n",
    "    image_line_removed = detect_line(image_threshold)\n",
    "    return image_line_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-d5f5191374b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mconfigs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:/Users/Purple/textrecognition/config1.yml'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Purple/textrecognition/chieng1.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\textrecognition\\src\\pre_process.py\u001b[0m in \u001b[0;36mprocess_image\u001b[1;34m(image_file)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[0mimage_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_gray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_origin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;31m# Morph Gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m     \u001b[0mimage_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_gray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m     \u001b[1;31m# Threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[0mimage_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_threshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_gradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\textrecognition\\src\\pre_process.py\u001b[0m in \u001b[0;36mget_gradient\u001b[1;34m(image_gray)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;31m# get configs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0mkernel_size_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gradient'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'kernel_size_row'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[0mkernel_size_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gradient'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'kernel_size_col'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;31m# make kernel matrix for dilation and erosion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "configs = 'C:/Users/Purple/textrecognition/config1.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John 1:1-51\\nIn the beginning was the Word, and the Word was with God.,\\nand the Word was God.\\n2 He was in the beginning with God.\\n3 All things were made through him, and without him was\\nnot any thing made that was made.\\n4 In him was life,and the life was the light of men.\\n5 The light shines in the darkness, and the darkness has not\\novercome it.\\n14 And the Word became flesh and dwelt among us, and we\\nhave seen his glory, glory as of the only Son from the Father,\\nfull of grace and truth.\\nAlJbiG, OSNER, OMEN.\\n2\\n3 DUEMHINEMN: MEM, .\\n4 RTMRREAMY.\\n5 XHERE, ReNTNESX.\\n14 (EfERI4id, %% f6eser®.\\nRINtMHIEMEX,\\nThere are many more examples that I can bring up to show that\\n\\nthe Chinese do have this long memory in their written\\nlanguage, customs and philosophies.\\n\\n \\n\\nHow wonderful it is to know that the Chinese have all along\\nknown this One and Only God throughout their unbroken\\nhistory.\\n\\nThe Word of God revealed to us that we have sinned and fallen\\nshort of the glory of God. We have been separated from God\\nbecause of our sin.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text_from_image(image):\n",
    "    \"\"\" OCR 엔진(tesseract) 를 이용해 이미지에서 글자를 추출합니다.\n",
    "\n",
    "    :param image: 텍스트(Text)를 추출할 resource 이미지\n",
    "    :return: 추출한 텍스트(Text)를 String 형으로 반환\n",
    "    \"\"\"\n",
    "    # todo language 도 configs.yml file 에서 설정할 수 있도록 변경하기\n",
    "    img = Image.open(open(image,'rb'))\n",
    "    text = ocr.image_to_string(img, lang='eng+chi')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-0649dcd0f375>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mread_text_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Purple/textrecognition/chieng1.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-96-0649dcd0f375>\u001b[0m in \u001b[0;36mread_text_from_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_text_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcropped_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcropped\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcropped_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\textrecognition\\src\\pre_process.py\u001b[0m in \u001b[0;36mprocess_image\u001b[1;34m(image_file)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[0mimage_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_gray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_origin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;31m# Morph Gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m     \u001b[0mimage_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_gray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m     \u001b[1;31m# Threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[0mimage_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_threshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_gradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\textrecognition\\src\\pre_process.py\u001b[0m in \u001b[0;36mget_gradient\u001b[1;34m(image_gray)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;31m# get configs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0mkernel_size_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gradient'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'kernel_size_row'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[0mkernel_size_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gradient'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'kernel_size_col'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;31m# make kernel matrix for dilation and erosion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-b1a383049025>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m \u001b[0mprocess_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Purple/textrecognition/chieng1.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-104-b1a383049025>\u001b[0m in \u001b[0;36mprocess_image\u001b[1;34m(image_file)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[0mimage_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_gray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_origin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;31m# Morph Gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m     \u001b[0mimage_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_gray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m     \u001b[1;31m# Threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[0mimage_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_threshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_gradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-104-b1a383049025>\u001b[0m in \u001b[0;36mget_gradient\u001b[1;34m(image_gray)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;31m# get configs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mkernel_size_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gradient'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'kernel_size_row'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[0mkernel_size_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gradient'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'kernel_size_col'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;31m# make kernel matrix for dilation and erosion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "def read_configs(config_file):\n",
    "    \"\"\" .yml file 을 읽어서 configuration 값의 객체를 갖습니다.\n",
    "\n",
    "    :param config_file:\n",
    "    :return: 읽은 configuration 을 담고있는 dictionary 형태로 반환\n",
    "    \"\"\"\n",
    "    # read contents from .yam config file\n",
    "    with open(config_file, 'r') as yml_file:\n",
    "        configurations = yaml.load(yml_file)  # use 'yaml' package to read .yml file\n",
    "\n",
    "    global configs  # global var : configs\n",
    "    configs = configurations  # set configs\n",
    "    return configurations  # return read configurations\n",
    "\n",
    "\n",
    "def print_configs():\n",
    "    \"\"\" 전역변수 configs 에 저장된 configuration 내용을 출력합니다.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    global configs  # refer global variable : configs\n",
    "    for section in configs:\n",
    "        print(section + \":\")\n",
    "        print(configs[section])\n",
    "\n",
    "\n",
    "def resize(image, flag=-1):\n",
    "    \"\"\" Configuration 의 width, height 값을 기준으로 이미지 사이즈를 변경합니다.\n",
    "\n",
    "    :param image - cv2 이미지 객체\n",
    "    :param flag - flag > 0 이면 사이즈를 증가, flag < 0 (default)이면 사이즈를 축소\n",
    "    :return: image_copy - 사이즈가 변환된 이미지\n",
    "    \"\"\"\n",
    "    # get configs\n",
    "    global configs\n",
    "    standard_height = configs['resize_origin']['standard_height']\n",
    "    standard_width = configs['resize_origin']['standard_width']\n",
    "    # get image size\n",
    "    height, width = image.shape[:2]\n",
    "    image_copy = image.copy()\n",
    "    # print original size (width, height)\n",
    "    print(\"origin (width : \" + str(width) + \", height : \" + str(height) + \")\")\n",
    "    rate = 1  # default\n",
    "    if (flag > 0 and height < standard_height) or (flag < 0 and height > standard_height):  # Resize based on height\n",
    "        rate = standard_height / height\n",
    "    elif (flag > 0 and width < standard_width) or (flag < 0 and height > standard_height):  # Resize based on width\n",
    "        rate = standard_width / width\n",
    "    # resize\n",
    "    w = round(width * rate)  # should be integer\n",
    "    h = round(height * rate)  # should be integer\n",
    "    image_copy = cv2.resize(image_copy, (w, h))\n",
    "    # print modified size (width, height)\n",
    "    print(\"after resize : (width : \" + str(w) + \", height : \" + str(h) + \")\")\n",
    "    return image_copy\n",
    "\n",
    "\n",
    "def open_original(file_path):\n",
    "    \"\"\" image file 을 읽어들여서 OpenCV image 객체로 반환합니다.\n",
    "\n",
    "    :param file_path:  경로를 포함한 이미지 파일\n",
    "    :return:  OpenCV 의 BGR image 객체 (3 dimension)\n",
    "    \"\"\"\n",
    "    image_origin = cv2.imread(file_path)  # read image from file\n",
    "    return image_origin\n",
    "\n",
    "\n",
    "def get_gray(image_origin):\n",
    "    \"\"\" image 객체를 인자로 받아서 Gray-scale 을 적용한 2차원 이미지 객체로 반환합니다.\n",
    "    이 때 인자로 입력되는 이미지는 BGR 컬러 이미지여야 합니다.\n",
    "\n",
    "    :param image_origin: OpenCV 의 BGR image 객체 (3 dimension)\n",
    "    :return: gray-scale 이 적용된 image 객체 (2 dimension)\n",
    "    \"\"\"\n",
    "    copy = image_origin.copy()  # copy the image to be processed\n",
    "    image_grey = cv2.cvtColor(copy, cv2.COLOR_BGR2GRAY)  # apply gray-scale to the image\n",
    "    return image_grey\n",
    "\n",
    "\n",
    "def get_canny(image_gray):\n",
    "    copy = image_gray.copy()\n",
    "    kernel_size = 5\n",
    "    blur_gray = cv2.GaussianBlur(copy, (kernel_size, kernel_size), 0)\n",
    "    low_threshold = 50\n",
    "    high_threshold = 150\n",
    "    edges = cv2.Canny(blur_gray, low_threshold, high_threshold)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def get_gradient(image_gray):\n",
    "    \"\"\" 이미지에 Dilation 과 Erosion 을 적용하여 그 차이를 이용해 윤곽선을 추출합니다.\n",
    "    이 때 인자로 입력되는 이미지는 Gray scale 이 적용된 2차원 이미지여야 합니다.\n",
    "\n",
    "    :param image_gray: Gray-scale 이 적용된 OpenCV image (2 dimension)\n",
    "    :return: 윤곽선을 추출한 결과 이미지 (OpenCV image)\n",
    "    \"\"\"\n",
    "    copy = image_gray.copy()  # copy the image to be processed\n",
    "    # get configs\n",
    "    global configs\n",
    "    kernel_size_row = configs['gradient']['kernel_size_row']\n",
    "    kernel_size_col = configs['gradient']['kernel_size_col']\n",
    "    # make kernel matrix for dilation and erosion\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size_row, kernel_size_col))\n",
    "    # morph gradient\n",
    "    image_gradient = cv2.morphologyEx(copy, cv2.MORPH_GRADIENT, kernel)\n",
    "    return image_gradient\n",
    "\n",
    "\n",
    "def remove_long_line(image_binary):\n",
    "    \"\"\" 이미지에서 직선을 찾아서 삭제합니다.\n",
    "    글자 경계를 찾을 때 방해가 되는 직선을 찾아서 삭제합니다.\n",
    "    이 때 인자로 입력되는 이미지 2 차원(2 dimension) 흑백(Binary) 이미지여야 합니다.\n",
    "    직선을 삭제할 때는 해당 라인을 검정색으로 그려 덮어 씌웁니다. \n",
    "\n",
    "    :param image_binary: 흑백(Binary) OpenCV image (2 dimension)\n",
    "    :return: 라인이 삭제된 이미지 (OpenCV image)\n",
    "    \"\"\"\n",
    "    copy = image_binary.copy()  # copy the image to be processed\n",
    "    # get configs\n",
    "    global configs\n",
    "    threshold = configs['remove_line']['threshold']\n",
    "    min_line_length = configs['remove_line']['min_line_length']\n",
    "    max_line_gap = configs['remove_line']['max_line_gap']\n",
    "\n",
    "    # find and remove lines\n",
    "    lines = cv2.HoughLinesP(copy, 1, np.pi / 180, threshold, np.array([]), min_line_length, max_line_gap)\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]  # get end point of line : ( (x1, y1) , (x2, y2) )\n",
    "            # slop = 0\n",
    "            # if x2 != x1:\n",
    "            #     slop = abs((y2-y1) / (x2-x1))\n",
    "            # if slop < 0.5 or slop > 50 or x2 == x1:  # only vertical or parallel lines.\n",
    "            # remove line drawing black line\n",
    "            cv2.line(copy, (x1, y1), (x2, y2), (0, 0, 0), 2)\n",
    "    return copy\n",
    "\n",
    "\n",
    "def get_threshold(image_gray):\n",
    "    \"\"\" 이미지에 Threshold 를 적용해서 흑백(Binary) 이미지객체를 반환합니다.\n",
    "    이 때 인자로 입력되는 이미지는 Gray-scale 이 적용된 2차원 이미지여야 합니다.\n",
    "    configs 에 적용된 threshold mode 에 따라 global threshold / mean adaptive threshold / gaussian adaptive threshold\n",
    "    를 적용할 수 있습니다.\n",
    "\n",
    "    :param image_gray: Gray-scale 이 적용된 OpenCV image (2 dimension)\n",
    "    :return: Threshold 를 적용한 흑백(Binary) 이미지\n",
    "    \"\"\"\n",
    "    copy = image_gray.copy()  # copy the image to be processed\n",
    "    # get configs\n",
    "    global configs\n",
    "    mode = configs['threshold']['mode']  # get threshold mode (mean or gaussian or global)\n",
    "    block_size = configs['threshold']['block_size']\n",
    "    subtract_val = configs['threshold']['subtract_val']\n",
    "\n",
    "    if mode == 'mean':  # adaptive threshold - mean\n",
    "        image_threshold = cv2.adaptiveThreshold(copy, 255, cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "                                                cv2.THRESH_BINARY_INV, block_size, subtract_val)\n",
    "    elif mode == 'gaussian':  # adaptive threshold - gaussian\n",
    "        image_threshold = cv2.adaptiveThreshold(copy, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                                cv2.THRESH_BINARY_INV, block_size, subtract_val)\n",
    "    else:  # (mode == 'global') global threshold - otsu's binary operation\n",
    "        image_threshold = get_otsu_threshold(copy)\n",
    "\n",
    "    return image_threshold  # Returns the image with the threshold applied.\n",
    "\n",
    "\n",
    "def get_global_threshold(image_gray, threshold_value=130):\n",
    "    \"\"\" 이미지에 Global Threshold 를 적용해서 흑백(Binary) 이미지객체를 반환합니다.\n",
    "    하나의 값(threshold_value)을 기준으로 이미지 전체에 적용하여 Threshold 를 적용합니다.\n",
    "    픽셀의 밝기 값이 기준 값 이상이면 흰색, 기준 값 이하이면 검정색을 적용합니다.\n",
    "    이 때 인자로 입력되는 이미지는 Gray-scale 이 적용된 2차원 이미지여야 합니다.\n",
    "    \n",
    "    :param image_gray:\n",
    "    :param threshold_value: 이미지 전체에 Threshold 를 적용할 기준 값.\n",
    "    :return: Global Threshold 를 적용한 흑백(Binary) 이미지\n",
    "    \"\"\"\n",
    "    copy = image_gray.copy()  # copy the image to be processed\n",
    "    _, binary_image = cv2.threshold(copy, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    return binary_image\n",
    "\n",
    "\n",
    "def get_otsu_threshold(image_gray):\n",
    "    \"\"\"  이미지에 Global Threshold 를 적용해서 흑백(Binary) 이미지객체를 반환합니다.\n",
    "    하나의 값을 기준으로 이미지 전체에 적용하여 Threshold 를 적용합니다.\n",
    "    해당 값은 Otsu's Binarization 에 의해 자동으로 이미지의 히스토그램을 분석한 후 중간값으로 설정됩니다.\n",
    "    픽셀의 밝기 값이 기준 값 이상이면 흰색, 기준 값 이하이면 검정색을 적용합니다.\n",
    "    이 때 인자로 입력되는 이미지는 Gray-scale 이 적용된 2차원 이미지여야 합니다.\n",
    "\n",
    "    :param image_gray: Gray-scale 이 적용된 OpenCV image (2 dimension)\n",
    "    :return: Otsu's Binarization에 의해 Global Threshold 를 적용한 흑백(Binary) 이미지\n",
    "    \"\"\"\n",
    "    copy = image_gray.copy()  # copy the image to be processed\n",
    "    blur = cv2.GaussianBlur(copy, (5, 5), 0)  # Gaussian blur 를 통해 noise 를 제거한 후\n",
    "    # global threshold with otsu's binarization\n",
    "    ret3, image_otsu = cv2.threshold(copy, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return image_otsu\n",
    "\n",
    "\n",
    "def get_closing(image_gray):\n",
    "    \"\"\" 이미지에 Morph Close 를 적용한 이미지객체를 반환합니다.\n",
    "    이미지에 Dilation 수행을 한 후 Erosion 을 수행한 것입니다.\n",
    "    이 때 인자로 입력되는 이미지는 Gray-scale 이 적용된 2차원 이미지여야 합니다.\n",
    "    configs 에 의해 kernel size 값을 설정할 수 있습니다.\n",
    "\n",
    "    :param image_gray: Gray-scale 이 적용된 OpenCV image (2 dimension)\n",
    "    :return: Morph Close 를 적용한 흑백(Binary) 이미지\n",
    "    \"\"\"\n",
    "    copy = image_gray.copy()  # copy the image to be processed\n",
    "    # get configs\n",
    "    global configs\n",
    "    kernel_size_row = configs['close']['kernel_size_row']\n",
    "    kernel_size_col = configs['close']['kernel_size_col']\n",
    "    # make kernel matrix for dilation and erosion\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size_row, kernel_size_col))\n",
    "    # closing (dilation and erosion)\n",
    "    image_close = cv2.morphologyEx(copy, cv2.MORPH_CLOSE, kernel)\n",
    "    return image_close\n",
    "\n",
    "\n",
    "def get_contours(image):\n",
    "    \"\"\" 이미지에서 Contour 를 추출하여 반환합니다.\n",
    "    Contour 추출 모드는 configs 에서 설정할 수 있습니다.\n",
    "    찾은 contour 리스트를 dictionary 형태로 반환합니다.\n",
    "    이미지 처리(Image processing) 단계를 거친 후 contour 를 잘 추출할 수 있습니다.\n",
    "\n",
    "    :param image: OpenCV의 image 객체 (2 dimension)\n",
    "    :return: 이미지에서 추출한 contours\n",
    "    \"\"\"\n",
    "    # get configs\n",
    "    global configs\n",
    "    retrieve_mode = configs['contour']['retrieve_mode']  # integer value\n",
    "    approx_method = configs['contour']['approx_method']  # integer value\n",
    "    # find contours from the image\n",
    "    _, contours, _ = cv2.findContours(image, retrieve_mode, approx_method)\n",
    "    return contours\n",
    "\n",
    "\n",
    "def draw_contour_rect(image_origin, contours):\n",
    "    \"\"\" 사각형의 Contour 를 이미지 위에 그려서 반환합니다.\n",
    "    찾은 Contours 를 기반으로 이미지 위에 각 contour 를 감싸는 외각 사각형을 그립니다.\n",
    "\n",
    "    :param image_origin: OpenCV의 image 객체\n",
    "    :param contours: 이미지 위에 그릴 contour 리스트\n",
    "    :return: 사각형의 Contour 를 그린 이미지\n",
    "    \"\"\"\n",
    "    rgb_copy = image_origin.copy()  # copy the image to be processed\n",
    "    # get configs\n",
    "    global configs\n",
    "    min_width = configs['contour']['min_width']\n",
    "    min_height = configs['contour']['min_height']\n",
    "    # Draw bounding rectangles\n",
    "    for contour in contours:\n",
    "        x, y, width, height = cv2.boundingRect(contour)  # top-left vertex coordinates (x,y) , width, height\n",
    "        # Draw screenshot that are larger than the standard size\n",
    "        if width > min_width and height > min_height:\n",
    "            cv2.rectangle(rgb_copy, (x, y), (x + width, y + height), (0, 255, 0), 2)\n",
    "\n",
    "    return rgb_copy\n",
    "\n",
    "\n",
    "def get_cropped_images(image_origin, contours):\n",
    "    \"\"\" 이미지에서 찾은 Contour 부분들을 잘라내어 반환합니다.\n",
    "    각 contour 를 감싸는 외각 사각형에 여유분(padding)을 주어 이미지를 잘라냅니다.\n",
    "\n",
    "    :param image_origin: 원본 이미지\n",
    "    :param contours: 잘라낼 contour 리스트\n",
    "    :return: contours 를 기반으로 잘라낸 이미지(OpenCV image 객체) 리스트\n",
    "    \"\"\"\n",
    "    image_copy = image_origin.copy()  # copy the image to be processed\n",
    "    # get configs\n",
    "    global configs\n",
    "    min_width = configs['contour']['min_width']\n",
    "    min_height = configs['contour']['min_height']\n",
    "    padding = 8  # to give the padding when cropping the screenshot\n",
    "    origin_height, origin_width = image_copy.shape[:2]  # get image size\n",
    "    cropped_images = []  # list to save the crop image.\n",
    "\n",
    "    for contour in contours:  # Crop the screenshot with on bounding rectangles of contours\n",
    "        x, y, width, height = cv2.boundingRect(contour)  # top-left vertex coordinates (x,y) , width, height\n",
    "        # screenshot that are larger than the standard size\n",
    "        if width > min_width and height > min_height:\n",
    "            # The range of row to crop (with padding)\n",
    "            row_from = (y - padding) if (y - padding) > 0 else y\n",
    "            row_to = (y + height + padding) if (y + height + padding) < origin_height else y + height\n",
    "            # The range of column to crop (with padding)\n",
    "            col_from = (x - padding) if (x - padding) > 0 else x\n",
    "            col_to = (x + width + padding) if (x + width + padding) < origin_width else x + width\n",
    "            # Crop the image with Numpy Array\n",
    "            cropped = image_copy[row_from: row_to, col_from: col_to]\n",
    "            cropped_images.append(cropped)  # add to the list\n",
    "    return cropped_images\n",
    "\n",
    "\n",
    "def save_image(image, name_prefix='untitled'):\n",
    "    \"\"\" 이미지(OpenCV image 객체)를 이미지파일(.jpg)로 저장합니다.\n",
    "\n",
    "    :param image: 저장할 이미지 (OpenCV image 객체)\n",
    "    :param name_prefix: 파일명을 식별할 접두어 (확장자 제외)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # make file name with the datetime suffix.\n",
    "    d_date = datetime.datetime.now()  # get current datetime\n",
    "    current_datetime = d_date.strftime(\"%Y%m%d%I%M%S\")  # datetime to string\n",
    "    file_path = name_prefix + '_'+ current_datetime + \".jpg\"  # complete file name\n",
    "    cv2.imwrite(file_path, image)\n",
    "\n",
    "\n",
    "def get_text_from_image(image):\n",
    "    \"\"\" OCR 엔진(tesseract) 를 이용해 이미지에서 글자를 추출합니다.\n",
    "\n",
    "    :param image: 텍스트(Text)를 추출할 resource 이미지\n",
    "    :return: 추출한 텍스트(Text)를 String 형으로 반환\n",
    "    \"\"\"\n",
    "    # todo language 도 configs.yml file 에서 설정할 수 있도록 변경하기\n",
    "    img = Image.open(open(image,'rb'))\n",
    "    text = ocr.image_to_string(img, lang='eng+chi')\n",
    "    return text\n",
    "\n",
    "\n",
    "def process_image(image_file):\n",
    "    \"\"\" 다섯 단계의 이미지 처리(Image precessing)를 힙니다.\n",
    "    현재 함수에서 순서를 변경하여 적용할 수 있습니다.\n",
    "    1) Gray-scale 적용\n",
    "    2) Morph Gradient 적용\n",
    "    3) Threshold 적용\n",
    "    4) Long Line Removal 적용\n",
    "    5) Close 적용\n",
    "    6) Contour 추출\n",
    "\n",
    "    :param image_file: 이미지 처리(Image precessing)를 적용할 이미지 파일\n",
    "    :return: 이미지 처리 후 글자로 추정되는 부분을 잘라낸 이미지 리스트\n",
    "    \"\"\"\n",
    "    image_origin = open_original(image_file)\n",
    "    # todo input 사이즈가 일정 수준 이하일 경우 cv2.pyrUp() 으로 사이즈를 확장할 수 있도록 자동화하기\n",
    "    # todo 아니면 설정파일에서 사이즈업 할지말지를 선택할 수 있도록 하기 (configs.yml)\n",
    "    # image_origin = cv2.pyrUp(image_origin)  # size up ( x4 )  이미지 크기가 작을 경우 이미지 사이즈업 해야합니다.\n",
    "    # Grey-Scale\n",
    "    image_gray = get_gray(image_origin)\n",
    "    # Morph Gradient\n",
    "    image_gradient = get_gradient(image_gray)\n",
    "    # Threshold\n",
    "    image_threshold = get_threshold(image_gradient)\n",
    "    # Long line remove\n",
    "    image_line_removed = remove_long_line(image_threshold)\n",
    "    # Morph Close\n",
    "    image_close = get_closing(image_line_removed)\n",
    "    contours = get_contours(image_close)\n",
    "\n",
    "    return get_cropped_images(image_origin, contours)  # 글자로 추정되는 부분을 잘라낸 이미지들을 반환\n",
    "\n",
    "def get_file_list(path):\n",
    "    \"\"\" path 가 가리키는 directory 의 모든 파일명을 읽어서 string 으로 반환합니다.\n",
    "    파일명은 Absolute path 가 포함된 이름입니다.\n",
    "\n",
    "    :param path: 읽어 들일 directory 의 절대경로\n",
    "    :return: directory 의 모든 file path 을 String 형으로 Array 에 담아 반환\n",
    "    \"\"\"\n",
    "    image_path_list = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        root_path = os.path.join(os.path.abspath(path), root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root_path, file)\n",
    "            image_path_list.append(file_path)\n",
    "\n",
    "    return image_path_list\n",
    "\n",
    "def read_text_from_image(image_path):\n",
    "    messages = []\n",
    "    cropped_images = pp.process_image(image_path)\n",
    "    count = 1\n",
    "    for cropped in cropped_images:\n",
    "        count += 1\n",
    "        # gray_copy = pp.get_gray(cropped)\n",
    "        # gradient_copy = pp.get_gradient(gray_copy)\n",
    "        # gradient_copy = cv2.cvtColor(gradient_copy, cv2.COLOR_GRAY2BGR)\n",
    "        # answer = jt.get_answer_from_cv2_Image(gradient_copy)\n",
    "        # print(answer)\n",
    "        msg = pp.get_text_from_image(cropped)\n",
    "        messages.append(msg)\n",
    "\n",
    "    return messages\n",
    "process_image('C:/Users/Purple/textrecognition/chieng1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-5b6ca5523cf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mread_text_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Purple/textrecognition/chieng1.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-49-685ec597c220>\u001b[0m in \u001b[0;36mread_text_from_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_text_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcropped_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcropped\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcropped_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\textrecognition\\src\\pre_process.py\u001b[0m in \u001b[0;36mprocess_image\u001b[1;34m(image_file)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[0mimage_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_gray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_origin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;31m# Morph Gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m     \u001b[0mimage_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_gray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m     \u001b[1;31m# Threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[0mimage_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_threshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_gradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\textrecognition\\src\\pre_process.py\u001b[0m in \u001b[0;36mget_gradient\u001b[1;34m(image_gray)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;31m# get configs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0mkernel_size_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gradient'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'kernel_size_row'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[0mkernel_size_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gradient'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'kernel_size_col'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;31m# make kernel matrix for dilation and erosion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "read_text_from_image('C:/Users/Purple/textrecognition/chieng1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2551\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2552\u001b[1;33m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2553\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-8c9e4613273a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# answer = jt.get_answer_from_cv2_Image(gradient_copy)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# print(answer)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcropped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\textrecognition\\src\\pre_process.py\u001b[0m in \u001b[0;36mget_text_from_image\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \"\"\"\n\u001b[0;32m    358\u001b[0m     \u001b[1;31m# todo language 도 configs.yml file 에서 설정할 수 있도록 변경하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mocr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'eng+chi'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2552\u001b[0m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2553\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2554\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2555\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "from src import pre_process as pp\n",
    "messages = []\n",
    "cropped_images = pp.process_image('C:/Users/Purple/textrecognition/chieng1.png')\n",
    "count = 1\n",
    "for cropped in cropped_images:\n",
    "    count += 1\n",
    "    # gray_copy = pp.get_gray(cropped)\n",
    "    # gradient_copy = pp.get_gradient(gray_copy)\n",
    "    # gradient_copy = cv2.cvtColor(gradient_copy, cv2.COLOR_GRAY2BGR)\n",
    "    # answer = jt.get_answer_from_cv2_Image(gradient_copy)\n",
    "    # print(answer)\n",
    "    msg = pp.get_text_from_image(cropped)\n",
    "    messages.append(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-e9f63fa0e357>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_text_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Purple/textrecognition/chieng1.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\textrecognition\\src\\pre_process.py\u001b[0m in \u001b[0;36mread_text_from_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_text_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m     \u001b[0mcropped_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcropped\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcropped_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pp' is not defined"
     ]
    }
   ],
   "source": [
    "pp.read_text_from_image('C:/Users/Purple/textrecognition/chieng1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-e9f63fa0e357>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_text_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Purple/textrecognition/chieng1.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\textrecognition\\src\\pre_process.py\u001b[0m in \u001b[0;36mread_text_from_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_text_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m     \u001b[0mcropped_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcropped\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcropped_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pp' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
