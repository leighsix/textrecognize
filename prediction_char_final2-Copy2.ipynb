{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "import yaml\n",
    "import datetime\n",
    "import pytesseract as ocr\n",
    "from PIL import Image\n",
    "from src import pre_process as pp\n",
    "import os\n",
    "\n",
    "# configurations to read from YAML file\n",
    "configs = None\n",
    "def read_configs(config_file):\n",
    "    \"\"\" .yml file 을 읽어서 configuration 값의 객체를 갖습니다.\n",
    "    :param config_file:\n",
    "    :return: 읽은 configuration 을 담고있는 dictionary 형태로 반환\n",
    "    \"\"\"\n",
    "    # read contents from .yam config file\n",
    "    with open(config_file, 'r') as yml_file:\n",
    "        configurations = yaml.load(yml_file)  # use 'yaml' package to read .yml file\n",
    "\n",
    "    global configs  # global var : configs\n",
    "    configs = configurations  # set configs\n",
    "    return configurations  # return read configurations\n",
    "\n",
    "read_configs('C:/Users/Purple/textrecognition/config.yml')\n",
    "\n",
    "def recognize_text_from_file(image_path):\n",
    "    model_full_path = 'C:/Users/Purple/textrecognition/model/learning_graph.pb'\n",
    "    labels_full_path = 'C:/Users/Purple/textrecognition/model/learning_labels.txt'\n",
    "    # Read in the image_data\n",
    "    image_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
    "\n",
    "    # Loads label file, strips off carriage return\n",
    "    label_lines = [line.rstrip() for line\n",
    "                   in tf.gfile.GFile(labels_full_path)]\n",
    "\n",
    "    # Unpersists graph from file\n",
    "    with tf.gfile.FastGFile(model_full_path, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        _ = tf.import_graph_def(graph_def, name='')\n",
    "    \n",
    "    # Unpersists graph from file\n",
    "    with tf.Session() as sess:\n",
    "        # Feed the image_data as input to the graph and get first prediction\n",
    "        softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\n",
    "        predictions = sess.run(softmax_tensor, {'DecodeJpeg/contents:0': image_data})\n",
    "\n",
    "        # Sort to show labels of first prediction in order of confidence\n",
    "        top_k = predictions[0].argsort()[-len(predictions[0]):][::-1]\n",
    "    # get most likely classification\n",
    "        print('=== prediction result ===')\n",
    "        for i in range(len(label_lines)):\n",
    "            name = label_lines[i]\n",
    "            score = predictions[0][i]\n",
    "            print('%s (%.2f%%)' % (name, score * 100))\n",
    "    answer = label_lines[top_k[0]]\n",
    "    return answer\n",
    "\n",
    "def get_language_from_file(image_path):\n",
    "    model_full_path = 'C:/Users/Purple/textrecognition/workspace/language_graph.pb'\n",
    "    labels_full_path = 'C:/Users/Purple/textrecognition/workspace/language_labels.txt'\n",
    "    # Read in the image_data\n",
    "    image_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
    "\n",
    "    # Loads label file, strips off carriage return\n",
    "    label_lines = [line.rstrip() for line\n",
    "                   in tf.gfile.GFile(labels_full_path)]\n",
    "\n",
    "    # Unpersists graph from file\n",
    "    with tf.gfile.FastGFile(model_full_path, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        _ = tf.import_graph_def(graph_def, name='')\n",
    "    \n",
    "    # Unpersists graph from file\n",
    "    with tf.Session() as sess:\n",
    "        # Feed the image_data as input to the graph and get first prediction\n",
    "        softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\n",
    "        predictions = sess.run(softmax_tensor, {'DecodeJpeg/contents:0': image_data})\n",
    "\n",
    "        # Sort to show labels of first prediction in order of confidence\n",
    "        top_k = predictions[0].argsort()[-len(predictions[0]):][::-1]\n",
    "    language = label_lines[top_k[0]]\n",
    "    return language\n",
    "\n",
    "def get_origin_text_from_file(image):\n",
    "    \"\"\" OCR 엔진(tesseract) 를 이용해 이미지에서 글자를 추출합니다.\n",
    "    :param image: 텍스트(Text)를 추출할 resource 이미지\n",
    "    :return: 추출한 텍스트(Text)를 String 형으로 반환\n",
    "    \"\"\"\n",
    "    # todo language 도 configs.yml file 에서 설정할 수 있도록 변경하기\n",
    "    img = Image.open(open(image,'rb'))\n",
    "    text = ocr.image_to_string(img, lang='eng+chi')\n",
    "    return text\n",
    "\n",
    "def get_text_from_file(image):\n",
    "    img = Image.open(open(image, 'rb'))\n",
    "    text =[]\n",
    "    if recognize_text_from_file(image) == 'text' :\n",
    "        text = ocr.image_to_string(img, lang='eng+chi_sim')\n",
    "    return text\n",
    "    \n",
    "    \n",
    "def get_textlanguage_from_file(image):\n",
    "    \"\"\" OCR 엔진(tesseract) 를 이용해 이미지에서 글자를 추출합니다.\n",
    "    :param image: 텍스트(Text)를 추출할 resource 이미지\n",
    "    :return: 추출한 텍스트(Text)를 String 형으로 반환\n",
    "    \"\"\"\n",
    "    # todo language 도 configs.yml file 에서 설정할 수 있도록 변경하기\n",
    "    img = Image.open(open(image, 'rb'))\n",
    "    text=[]\n",
    "    if recognize_text_from_file(image) == 'text' :\n",
    "        if get_language_from_file(image) == 'eng' :\n",
    "            text = ocr.image_to_string(img, lang='eng')\n",
    "        elif get_language_from_file(image) == 'chi' :\n",
    "            text = ocr.image_to_string(img, lang='chi')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import yaml\n",
    "import datetime\n",
    "import pytesseract as ocr\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def resize(image, flag=-1):\n",
    "    \"\"\" Configuration 의 width, height 값을 기준으로 이미지 사이즈를 변경합니다.\n",
    "\n",
    "    :param image - cv2 이미지 객체\n",
    "    :param flag - flag > 0 이면 사이즈를 증가, flag < 0 (default)이면 사이즈를 축소\n",
    "    :return: image_copy - 사이즈가 변환된 이미지\n",
    "    \"\"\"\n",
    "    # get configs\n",
    "    global configs\n",
    "    standard_height = configs['resize_origin']['standard_height']\n",
    "    standard_width = configs['resize_origin']['standard_width']\n",
    "    # get image size\n",
    "    height, width = image.shape[:2]\n",
    "    image_copy = image.copy()\n",
    "    # print original size (width, height)\n",
    "    print(\"origin (width : \" + str(width) + \", height : \" + str(height) + \")\")\n",
    "    rate = 1  # default\n",
    "    if (flag > 0 and height < standard_height) or (flag < 0 and height > standard_height):  # Resize based on height\n",
    "        rate = standard_height / height\n",
    "    elif (flag > 0 and width < standard_width) or (flag < 0 and height > standard_height):  # Resize based on width\n",
    "        rate = standard_width / width\n",
    "    # resize\n",
    "    w = round(width * rate)  # should be integer\n",
    "    h = round(height * rate)  # should be integer\n",
    "    image_copy = cv2.resize(image_copy, (w, h))\n",
    "    # print modified size (width, height)\n",
    "    print(\"after resize : (width : \" + str(w) + \", height : \" + str(h) + \")\")\n",
    "    return image_copy\n",
    "\n",
    "\n",
    "def open_original(file_path):\n",
    "    \"\"\" image file 을 읽어들여서 OpenCV image 객체로 반환합니다.\n",
    "\n",
    "    :param file_path:  경로를 포함한 이미지 파일\n",
    "    :return:  OpenCV 의 BGR image 객체 (3 dimension)\n",
    "    \"\"\"\n",
    "    image_origin = cv2.imread(file_path)  # read image from file\n",
    "    return image_origin\n",
    "\n",
    "\n",
    "def get_gray(image_origin):\n",
    "    \"\"\" image 객체를 인자로 받아서 Gray-scale 을 적용한 2차원 이미지 객체로 반환합니다.\n",
    "    이 때 인자로 입력되는 이미지는 BGR 컬러 이미지여야 합니다.\n",
    "\n",
    "    :param image_origin: OpenCV 의 BGR image 객체 (3 dimension)\n",
    "    :return: gray-scale 이 적용된 image 객체 (2 dimension)\n",
    "    \"\"\"\n",
    "    copy = image_origin.copy()  # copy the image to be processed\n",
    "    image_grey = cv2.cvtColor(copy, cv2.COLOR_BGR2GRAY)  # apply gray-scale to the image\n",
    "    return image_grey\n",
    "\n",
    "\n",
    "def get_canny(image_gray):\n",
    "    copy = image_gray.copy()\n",
    "    kernel_size = 5\n",
    "    blur_gray = cv2.GaussianBlur(copy, (kernel_size, kernel_size), 0)\n",
    "    low_threshold = 50\n",
    "    high_threshold = 150\n",
    "    edges = cv2.Canny(blur_gray, low_threshold, high_threshold)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def get_gradient(image_gray):\n",
    "    copy = image_gray.copy()  # copy the image to be processed\n",
    "    # get configs\n",
    "    global configs\n",
    "    kernel_size_row = configs['gradient']['kernel_size_row']\n",
    "    kernel_size_col = configs['gradient']['kernel_size_col']\n",
    "    # make kernel matrix for dilation and erosion\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size_row, kernel_size_col))\n",
    "    # morph gradient\n",
    "    image_gradient = cv2.morphologyEx(copy, cv2.MORPH_GRADIENT, kernel)\n",
    "    return image_gradient\n",
    "\n",
    "\n",
    "def remove_long_line(image_binary):\n",
    "    copy = image_binary.copy()  # copy the image to be processed\n",
    "    # get configs\n",
    "    global configs\n",
    "    threshold = configs['remove_line']['threshold']\n",
    "    min_line_length = configs['remove_line']['min_line_length']\n",
    "    max_line_gap = configs['remove_line']['max_line_gap']\n",
    "\n",
    "    # find and remove lines\n",
    "    lines = cv2.HoughLinesP(copy, 1, np.pi / 180, threshold, min_line_length, max_line_gap)\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            slop = 0\n",
    "            for x1, y1, x2, y2 in line :  # get end point of line : ( (x1, y1) , (x2, y2) )\n",
    "                if x2 == x1 :\n",
    "                    cv2.line(copy, (x1, y1), (x2, y2), (0, 0, 0), 2)   # remove line drawing black line\n",
    "                elif x2 != x1:\n",
    "                    slop = abs((y2-y1) / (x2-x1))\n",
    "                    if slop > 50 :  # only vertical lines.\n",
    "                        cv2.line(copy, (x1, y1), (x2, y2), (0, 0, 0), 2)   # remove line drawing black line\n",
    "    return copy\n",
    "\n",
    "\n",
    "def get_threshold(image_gray):\n",
    "    copy = image_gray.copy()  # copy the image to be processed\n",
    "    # get configs\n",
    "    global configs\n",
    "    mode = configs['threshold']['mode']  # get threshold mode (mean or gaussian or global)\n",
    "    block_size = configs['threshold']['block_size']\n",
    "    subtract_val = configs['threshold']['subtract_val']\n",
    "\n",
    "    if mode == 'mean':  # adaptive threshold - mean\n",
    "        image_threshold = cv2.adaptiveThreshold(copy, 255, cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "                                                cv2.THRESH_BINARY_INV, block_size, subtract_val)\n",
    "    elif mode == 'gaussian':  # adaptive threshold - gaussian\n",
    "        image_threshold = cv2.adaptiveThreshold(copy, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                                cv2.THRESH_BINARY_INV, block_size, subtract_val)\n",
    "    else:  # (mode == 'global') global threshold - otsu's binary operation\n",
    "        image_threshold = get_otsu_threshold(copy)\n",
    "\n",
    "    return image_threshold  # Returns the image with the threshold applied.\n",
    "\n",
    "\n",
    "def get_global_threshold(image_gray, threshold_value=130):\n",
    "    \"\"\" 이미지에 Global Threshold 를 적용해서 흑백(Binary) 이미지객체를 반환합니다.\n",
    "    하나의 값(threshold_value)을 기준으로 이미지 전체에 적용하여 Threshold 를 적용합니다.\n",
    "    픽셀의 밝기 값이 기준 값 이상이면 흰색, 기준 값 이하이면 검정색을 적용합니다.\n",
    "    이 때 인자로 입력되는 이미지는 Gray-scale 이 적용된 2차원 이미지여야 합니다.\n",
    "    \n",
    "    :param image_gray:\n",
    "    :param threshold_value: 이미지 전체에 Threshold 를 적용할 기준 값.\n",
    "    :return: Global Threshold 를 적용한 흑백(Binary) 이미지\n",
    "    \"\"\"\n",
    "    copy = image_gray.copy()  # copy the image to be processed\n",
    "    _, binary_image = cv2.threshold(copy, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    return binary_image\n",
    "\n",
    "\n",
    "def get_otsu_threshold(image_gray):\n",
    "    \"\"\"  이미지에 Global Threshold 를 적용해서 흑백(Binary) 이미지객체를 반환합니다.\n",
    "    하나의 값을 기준으로 이미지 전체에 적용하여 Threshold 를 적용합니다.\n",
    "    해당 값은 Otsu's Binarization 에 의해 자동으로 이미지의 히스토그램을 분석한 후 중간값으로 설정됩니다.\n",
    "    픽셀의 밝기 값이 기준 값 이상이면 흰색, 기준 값 이하이면 검정색을 적용합니다.\n",
    "    이 때 인자로 입력되는 이미지는 Gray-scale 이 적용된 2차원 이미지여야 합니다.\n",
    "\n",
    "    :param image_gray: Gray-scale 이 적용된 OpenCV image (2 dimension)\n",
    "    :return: Otsu's Binarization에 의해 Global Threshold 를 적용한 흑백(Binary) 이미지\n",
    "    \"\"\"\n",
    "    copy = image_gray.copy()  # copy the image to be processed\n",
    "    blur = cv2.GaussianBlur(copy, (5, 5), 0)  # Gaussian blur 를 통해 noise 를 제거한 후\n",
    "    # global threshold with otsu's binarization\n",
    "    ret3, image_otsu = cv2.threshold(copy, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return image_otsu\n",
    "\n",
    "\n",
    "def get_closing(image_gray):\n",
    "    \"\"\" 이미지에 Morph Close 를 적용한 이미지객체를 반환합니다.\n",
    "    이미지에 Dilation 수행을 한 후 Erosion 을 수행한 것입니다.\n",
    "    이 때 인자로 입력되는 이미지는 Gray-scale 이 적용된 2차원 이미지여야 합니다.\n",
    "    configs 에 의해 kernel size 값을 설정할 수 있습니다.\n",
    "\n",
    "    :param image_gray: Gray-scale 이 적용된 OpenCV image (2 dimension)\n",
    "    :return: Morph Close 를 적용한 흑백(Binary) 이미지\n",
    "    \"\"\"\n",
    "    copy = image_gray.copy()  # copy the image to be processed\n",
    "    # get configs\n",
    "    global configs\n",
    "    kernel_size_row = configs['close']['kernel_size_row']\n",
    "    kernel_size_col = configs['close']['kernel_size_col']\n",
    "    # make kernel matrix for dilation and erosion\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size_row, kernel_size_col))\n",
    "    # closing (dilation and erosion)\n",
    "    image_close = cv2.morphologyEx(copy, cv2.MORPH_CLOSE, kernel)\n",
    "    return image_close\n",
    "\n",
    "\n",
    "def get_contours(image):\n",
    "    \"\"\" 이미지에서 Contour 를 추출하여 반환합니다.\n",
    "    Contour 추출 모드는 configs 에서 설정할 수 있습니다.\n",
    "    찾은 contour 리스트를 dictionary 형태로 반환합니다.\n",
    "    이미지 처리(Image processing) 단계를 거친 후 contour 를 잘 추출할 수 있습니다.\n",
    "\n",
    "    :param image: OpenCV의 image 객체 (2 dimension)\n",
    "    :return: 이미지에서 추출한 contours\n",
    "    \"\"\"\n",
    "    # get configs\n",
    "    global configs\n",
    "    retrieve_mode = configs['contour']['retrieve_mode']  # integer value\n",
    "    approx_method = configs['contour']['approx_method']  # integer value\n",
    "    # find contours from the image\n",
    "    _, contours, _ = cv2.findContours(image, retrieve_mode, approx_method)\n",
    "    return contours\n",
    "\n",
    "\n",
    "def draw_contour_rect(image_origin, contours):\n",
    "    \"\"\" 사각형의 Contour 를 이미지 위에 그려서 반환합니다.\n",
    "    찾은 Contours 를 기반으로 이미지 위에 각 contour 를 감싸는 외각 사각형을 그립니다.\n",
    "\n",
    "    :param image_origin: OpenCV의 image 객체\n",
    "    :param contours: 이미지 위에 그릴 contour 리스트\n",
    "    :return: 사각형의 Contour 를 그린 이미지\n",
    "    \"\"\"\n",
    "    rgb_copy = image_origin.copy()  # copy the image to be processed\n",
    "    # get configs\n",
    "    global configs\n",
    "    min_width = configs['contour']['min_width']\n",
    "    min_height = configs['contour']['min_height']\n",
    "    origin_height, origin_width = rgb_copy.shape[:2]  # get image size\n",
    "    padding = 5\n",
    "    # Draw bounding rectangles\n",
    "    for contour in contours:\n",
    "        x, y, width, height = cv2.boundingRect(contour)  # top-left vertex coordinates (x,y) , width, height\n",
    "        # Draw screenshot that are larger than the standard size\n",
    "        if width > min_width and height > min_height:\n",
    "            row_from = (y - padding) if (y - padding) > 0 else y\n",
    "            row_to = (y + height + padding) if (y + height + padding) < origin_height else y + height\n",
    "            # The range of column to crop (with padding)\n",
    "            col_from = (x - padding) if (x - padding) > 0 else x\n",
    "            col_to = (x + width + padding) if (x + width + padding) < origin_width else x + width\n",
    "            \n",
    "            cv2.rectangle(rgb_copy, (col_from, row_from), (col_to, row_to), (0, 255, 0), 2)\n",
    "\n",
    "    return rgb_copy\n",
    "\n",
    "\n",
    "def get_cropped_images(image_origin, contours):\n",
    "    image_copy = image_origin.copy()  # copy the image to be processed\n",
    "    # get configs\n",
    "    global configs\n",
    "    min_width = configs['contour']['min_width']\n",
    "    min_height = configs['contour']['min_height']\n",
    "    padding = 8  # to give the padding when cropping the screenshot\n",
    "    origin_height, origin_width = image_copy.shape[:2]  # get image size\n",
    "    cropped_images = []  # list to save the crop image.\n",
    "\n",
    "    for contour in contours:  # Crop the screenshot with on bounding rectangles of contours\n",
    "        x, y, width, height = cv2.boundingRect(contour)  # top-left vertex coordinates (x,y) , width, height\n",
    "        # screenshot that are larger than the standard size\n",
    "        if width > min_width and height > min_height:\n",
    "            # The range of row to crop (with padding)\n",
    "            row_from = (y - padding) if (y - padding) > 0 else y\n",
    "            row_to = (y + height + padding) if (y + height + padding) < origin_height else y + height\n",
    "            # The range of column to crop (with padding)\n",
    "            col_from = (x - padding) if (x - padding) > 0 else x\n",
    "            col_to = (x + width + padding) if (x + width + padding) < origin_width else x + width\n",
    "            # Crop the image with Numpy Array\n",
    "            cropped = image_copy[row_from: row_to, col_from: col_to]\n",
    "            cropped_images.append(cropped)  # add to the list\n",
    "    return cropped_images\n",
    "\n",
    "\n",
    "def save_image(image, name_prefix='untitled'):\n",
    "    \"\"\" 이미지(OpenCV image 객체)를 이미지파일(.jpg)로 저장합니다.\n",
    "\n",
    "    :param image: 저장할 이미지 (OpenCV image 객체)\n",
    "    :param name_prefix: 파일명을 식별할 접두어 (확장자 제외)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # make file name with the datetime suffix.\n",
    "    d_date = datetime.datetime.now()  # get current datetime\n",
    "    current_datetime = d_date.strftime(\"%Y%m%d%I%M%S\")  # datetime to string\n",
    "    file_path = name_prefix + '_'+ current_datetime + \".jpg\"  # complete file name\n",
    "    cv2.imwrite(file_path, image)\n",
    "\n",
    "\n",
    "\n",
    "def process_image(image_file):\n",
    "    \"\"\" 다섯 단계의 이미지 처리(Image precessing)를 힙니다.\n",
    "    현재 함수에서 순서를 변경하여 적용할 수 있습니다.\n",
    "    1) Gray-scale 적용\n",
    "    2) Morph Gradient 적용\n",
    "    3) Threshold 적용\n",
    "    4) Close 적용\n",
    "    5) Long Line Removal 적용\n",
    "    6) Contour 추출\n",
    "\n",
    "    :param image_file: 이미지 처리(Image precessing)를 적용할 이미지 파일\n",
    "    :return: 이미지 처리 후 글자로 추정되는 부분을 잘라낸 이미지 리스트\n",
    "    \"\"\"\n",
    "    image_origin = open_original(image_file)\n",
    "    # todo input 사이즈가 일정 수준 이하일 경우 cv2.pyrUp() 으로 사이즈를 확장할 수 있도록 자동화하기\n",
    "    # todo 아니면 설정파일에서 사이즈업 할지말지를 선택할 수 있도록 하기 (configs.yml)\n",
    "    # image_origin = cv2.pyrUp(image_origin)  # size up ( x4 )  이미지 크기가 작을 경우 이미지 사이즈업 해야합니다.\n",
    "    # Grey-Scale\n",
    "    image_gray = get_gray(image_origin)\n",
    "    # Morph Gradient\n",
    "    image_gradient = get_gradient(image_gray)\n",
    "    # Threshold\n",
    "    image_threshold = get_threshold(image_gradient)\n",
    "    # Morph Close\n",
    "    image_close = get_closing(image_threshold)\n",
    "    # Long line remove\n",
    "    image_line_removed = remove_long_line(image_close)\n",
    "\n",
    "    contours = get_contours(image_line_removed)\n",
    "\n",
    "    return get_cropped_images(image_origin, contours)  # 글자로 추정되는 부분을 잘라낸 이미지들을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image, name_prefix='untitled'):\n",
    "    # make file name with the datetime suffix.\n",
    "    d_date = datetime.datetime.now()  # get current datetime\n",
    "    current_datetime = d_date.strftime(\"%Y%m%d%I%M%S\")  # datetime to string\n",
    "    file_path = name_prefix + '_'+ current_datetime + \".jpg\"  # complete file name\n",
    "    cv2.imwrite(file_path, image)\n",
    "    \n",
    "def get_file_list(path):\n",
    "    image_path_list = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        root_path = os.path.join(os.path.abspath(path), root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root_path, file)\n",
    "            image_path_list.append(file_path)\n",
    "    return image_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'** The NEW\\nWOXFORD\\n\\nEnglish-Chinese\\nDICTIONARY\\n\\n     \\n\\nFTX DMX in] th.\\n\\n*\\n* oo0%%\\nAeon Cio rae s e Tal\\nea 18 \"n\\n\\n   \\n  \\n\\n\\'L.§9I‘h§1§ﬂﬂﬁ&\\n\\nThe World\\'s Most Trusted'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_origin_text_from_file('C:/Users/Purple/textrecognition/chieng3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import pre_process as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images = process_image('C:/Users/Purple/textrecognition/chieng3.png')\n",
    "count = 0\n",
    "for crop_image in cropped_images:\n",
    "    count += 1\n",
    "    save_image(crop_image, \"crop_\" + str(count))\n",
    "def save_image(image, name_prefix='untitled'):\n",
    "    # make file name with the datetime suffix.\n",
    "    d_date = datetime.datetime.now()  # get current datetime\n",
    "    current_datetime = d_date.strftime(\"%Y%m%d%I%M%S\")  # datetime to string\n",
    "    file_path = name_prefix + '_'+ current_datetime + \".jpg\"  # complete file name\n",
    "    cv2.imwrite(file_path, image)\n",
    "    \n",
    "def get_file_list(path):\n",
    "    image_path_list = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        root_path = os.path.join(os.path.abspath(path), root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root_path, file)\n",
    "            image_path_list.append(file_path)\n",
    "    return image_path_list\n",
    "def get_text_from_file(image):\n",
    "    img = Image.open(open(image, 'rb'))\n",
    "    text =[]\n",
    "    if recognize_text_from_file(image) == 'text' :\n",
    "        text = ocr.image_to_string(img, lang='eng+chi_sim')\n",
    "    return text\n",
    "def read_text_from_image(path):\n",
    "    messages = []\n",
    "    path = 'C:/Users/Purple/textrecognition/recog'\n",
    "    for filename in get_file_list(path):\n",
    "        msg = get_text_from_file(filename)\n",
    "        messages.append(msg)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-14fce7a9be77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mread_text_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Purple/textrecognition/chieng3.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-1f7025aaf89f>\u001b[0m in \u001b[0;36mread_text_from_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:/Users/Purple/textrecognition/recog'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mget_file_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_text_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-92637fb8f55a>\u001b[0m in \u001b[0;36mget_text_from_file\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrecognize_text_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'text'\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mocr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'eng+chi_sim'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pytesseract\\pytesseract.py\u001b[0m in \u001b[0;36mimage_to_string\u001b[1;34m(image, lang, config, nice, boxes, output_type)\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pytesseract\\pytesseract.py\u001b[0m in \u001b[0;36mrun_and_get_output\u001b[1;34m(image, extension, lang, config, nice, return_bytes)\u001b[0m\n\u001b[0;32m    138\u001b[0m         }\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mrun_tesseract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'output_filename_base'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextsep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pytesseract\\pytesseract.py\u001b[0m in \u001b[0;36mrun_tesseract\u001b[1;34m(input_filename, output_filename_base, extension, lang, config, nice)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m     \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout, endtime)\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m                 result = _winapi.WaitForSingleObject(self._handle,\n\u001b[1;32m-> 1055\u001b[1;33m                                                     timeout_millis)\n\u001b[0m\u001b[0;32m   1056\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWAIT_TIMEOUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "read_text_from_image('C:/Users/Purple/textrecognition/chieng3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_text_from_imagelist(image_path):\n",
    "    messages = []\n",
    "    path = 'C:/Users/Purple/textrecognition/recog'\n",
    "    for filename in get_file_list(path):\n",
    "        msg = recognize_text_from_file(filename)\n",
    "        messages.append(msg)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'text',\n",
       " 'text',\n",
       " 'text',\n",
       " 'text',\n",
       " 'text',\n",
       " 'text',\n",
       " 'nontext',\n",
       " 'nontext',\n",
       " 'text',\n",
       " 'text',\n",
       " 'nontext',\n",
       " 'nontext',\n",
       " 'text',\n",
       " 'text',\n",
       " 'text',\n",
       " 'text',\n",
       " 'nontext',\n",
       " 'nontext',\n",
       " 'nontext',\n",
       " 'nontext',\n",
       " 'text',\n",
       " 'nontext',\n",
       " 'nontext',\n",
       " 'text',\n",
       " 'text',\n",
       " 'text',\n",
       " 'text',\n",
       " 'text',\n",
       " 'text',\n",
       " 'text',\n",
       " 'text',\n",
       " 'text',\n",
       " 'text']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_imagelist('C:/Users/Purple/textrecognition/chieng3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (2.01%)\n",
      "text (97.99%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_1_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (2.30%)\n",
      "text (97.70%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_2_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (1.79%)\n",
      "text (98.21%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_3_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (6.67%)\n",
      "text (93.33%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_4_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (2.10%)\n",
      "text (97.90%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_5_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (12.79%)\n",
      "text (87.21%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_6_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (6.05%)\n",
      "text (93.95%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_7_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (11.57%)\n",
      "text (88.43%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_8_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (4.45%)\n",
      "text (95.55%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_9_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (4.21%)\n",
      "text (95.79%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_10_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (2.62%)\n",
      "text (97.38%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_11_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (2.93%)\n",
      "text (97.07%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_12_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (4.43%)\n",
      "text (95.57%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_13_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (2.71%)\n",
      "text (97.29%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_14_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (2.22%)\n",
      "text (97.78%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_15_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (0.40%)\n",
      "text (99.60%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_16_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (80.06%)\n",
      "text (19.94%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nontext'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_17_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (79.13%)\n",
      "text (20.87%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nontext'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_18_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (1.11%)\n",
      "text (98.89%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_19_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (55.62%)\n",
      "text (44.38%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nontext'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_20_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (84.86%)\n",
      "text (15.14%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nontext'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_21_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (28.33%)\n",
      "text (71.67%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_22_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (46.94%)\n",
      "text (53.06%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_23_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (0.16%)\n",
      "text (99.84%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_24_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (0.19%)\n",
      "text (99.81%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_25_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (84.22%)\n",
      "text (15.78%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nontext'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_26_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (64.27%)\n",
      "text (35.73%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nontext'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_27_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (61.21%)\n",
      "text (38.79%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nontext'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_28_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (78.70%)\n",
      "text (21.30%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nontext'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_29_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (73.75%)\n",
      "text (26.25%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nontext'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_30_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (82.38%)\n",
      "text (17.62%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nontext'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_31_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (0.96%)\n",
      "text (99.04%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_32_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (1.01%)\n",
      "text (98.99%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_33_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 예측 결과 ===\n",
      "nontext (42.13%)\n",
      "text (57.87%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_text_from_file('C:/Users/Purple/textrecognition/recog/crop_34_20180604054428.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
